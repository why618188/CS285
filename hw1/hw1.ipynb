{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "})\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Average Return</th>\n",
       "      <th>Std Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>416.858948</td>\n",
       "      <td>94.714958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>315.301178</td>\n",
       "      <td>49.690742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>376.114746</td>\n",
       "      <td>74.060997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>317.087647</td>\n",
       "      <td>40.026886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>267.312683</td>\n",
       "      <td>49.763527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0500</td>\n",
       "      <td>298.503052</td>\n",
       "      <td>63.166382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Average Return  Std Return\n",
       "0         0.0001      416.858948   94.714958\n",
       "1         0.0005      315.301178   49.690742\n",
       "2         0.0010      376.114746   74.060997\n",
       "3         0.0050      317.087647   40.026886\n",
       "4         0.0100      267.312683   49.763527\n",
       "5         0.0500      298.503052   63.166382"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('lr.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "# plt.tight_layout()\n",
    "plt.figure(figsize=(5.7, 3))\n",
    "ax = sns.lineplot(data=data, x='Learning Rate', y='Average Return')\n",
    "ax.errorbar(x=data['Learning Rate'].to_numpy(), y=data['Average Return'].to_numpy(), yerr=data['Std Return'], uplims=True, lolims=True, fmt='none', color='b')\n",
    "ax.set(xscale='log')\n",
    "plt.savefig('soln_pdf\\lr.pgf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret input 'Step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c5745e696db3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpointplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Step'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'AvgReturn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AvgReturn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'StdReturn'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vince\\.virtualenvs\\hw1-opyqaop4\\lib\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vince\\.virtualenvs\\hw1-opyqaop4\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mpointplot\u001b[1;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, markers, linestyles, dodge, join, scale, orient, color, palette, errwidth, capsize, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3363\u001b[0m                             \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3364\u001b[0m                             \u001b[0mmarkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdodge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3365\u001b[1;33m                             orient, color, palette, errwidth, capsize)\n\u001b[0m\u001b[0;32m   3366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vince\\.virtualenvs\\hw1-opyqaop4\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, markers, linestyles, dodge, join, scale, orient, color, palette, errwidth, capsize)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[1;32m-> 1656\u001b[1;33m                                  order, hue_order, units)\n\u001b[0m\u001b[0;32m   1657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vince\\.virtualenvs\\hw1-opyqaop4\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[1;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Could not interpret input '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;31m# Figure out the plotting orientation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret input 'Step'"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "ax = sns.pointplot(data=data, x='Step', y='AvgReturn', ci=None)\n",
    "# ax.errorbar(x=data['Step'].to_numpy(), y=data['AvgReturn'].to_numpy(), yerr=data['StdReturn'].to_numpy(), fmt='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_08-11-2020_00-53-12\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.06952838, dtype=float32)}\n",
      "{'Training Loss': array(0.05201672, dtype=float32)}\n",
      "{'Training Loss': array(0.04535733, dtype=float32)}\n",
      "{'Training Loss': array(0.04401592, dtype=float32)}\n",
      "{'Training Loss': array(0.04418095, dtype=float32)}\n",
      "{'Training Loss': array(0.04315159, dtype=float32)}\n",
      "{'Training Loss': array(0.03660894, dtype=float32)}\n",
      "{'Training Loss': array(0.0372747, dtype=float32)}\n",
      "{'Training Loss': array(0.03777801, dtype=float32)}\n",
      "{'Training Loss': array(0.03628476, dtype=float32)}\n",
      "{'Training Loss': array(0.03276211, dtype=float32)}\n",
      "{'Training Loss': array(0.03113788, dtype=float32)}\n",
      "{'Training Loss': array(0.03218365, dtype=float32)}\n",
      "{'Training Loss': array(0.03641711, dtype=float32)}\n",
      "{'Training Loss': array(0.02933824, dtype=float32)}\n",
      "{'Training Loss': array(0.03393339, dtype=float32)}\n",
      "{'Training Loss': array(0.02745186, dtype=float32)}\n",
      "{'Training Loss': array(0.03003899, dtype=float32)}\n",
      "{'Training Loss': array(0.02598349, dtype=float32)}\n",
      "{'Training Loss': array(0.03172781, dtype=float32)}\n",
      "{'Training Loss': array(0.03060216, dtype=float32)}\n",
      "{'Training Loss': array(0.02884076, dtype=float32)}\n",
      "{'Training Loss': array(0.03153071, dtype=float32)}\n",
      "{'Training Loss': array(0.02827151, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 376.11474609375\n",
      "Eval_StdReturn : 74.06099700927734\n",
      "Eval_MaxReturn : 590.7828979492188\n",
      "Eval_MinReturn : 220.7369842529297\n",
      "Eval_AverageEpLen : 69.16438356164383\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 84.08697319030762\n",
      "Training Loss : 0.03038956969976425\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.001 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_08-11-2020_00-54-38\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.14077036, dtype=float32)}\n",
      "{'Training Loss': array(0.12820484, dtype=float32)}\n",
      "{'Training Loss': array(0.13151236, dtype=float32)}\n",
      "{'Training Loss': array(0.13170055, dtype=float32)}\n",
      "{'Training Loss': array(0.12956926, dtype=float32)}\n",
      "{'Training Loss': array(0.11246548, dtype=float32)}\n",
      "{'Training Loss': array(0.12247079, dtype=float32)}\n",
      "{'Training Loss': array(0.12907282, dtype=float32)}\n",
      "{'Training Loss': array(0.13346878, dtype=float32)}\n",
      "{'Training Loss': array(0.10991907, dtype=float32)}\n",
      "{'Training Loss': array(0.11818689, dtype=float32)}\n",
      "{'Training Loss': array(0.11632851, dtype=float32)}\n",
      "{'Training Loss': array(0.11375931, dtype=float32)}\n",
      "{'Training Loss': array(0.12257476, dtype=float32)}\n",
      "{'Training Loss': array(0.11161118, dtype=float32)}\n",
      "{'Training Loss': array(0.13522515, dtype=float32)}\n",
      "{'Training Loss': array(0.12000896, dtype=float32)}\n",
      "{'Training Loss': array(0.11911001, dtype=float32)}\n",
      "{'Training Loss': array(0.11878126, dtype=float32)}\n",
      "{'Training Loss': array(0.12577124, dtype=float32)}\n",
      "{'Training Loss': array(0.11824274, dtype=float32)}\n",
      "{'Training Loss': array(0.11243878, dtype=float32)}\n",
      "{'Training Loss': array(0.12409816, dtype=float32)}\n",
      "{'Training Loss': array(0.12122223, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 267.31268310546875\n",
      "Eval_StdReturn : 49.763526916503906\n",
      "Eval_MaxReturn : 494.3653869628906\n",
      "Eval_MinReturn : 203.1709747314453\n",
      "Eval_AverageEpLen : 49.11764705882353\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 84.08897566795349\n",
      "Training Loss : 0.11223320662975311\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.01 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_07-11-2020_23-18-18\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.14325714, dtype=float32)}\n",
      "{'Training Loss': array(0.0955845, dtype=float32)}\n",
      "{'Training Loss': array(0.07178822, dtype=float32)}\n",
      "{'Training Loss': array(0.06261905, dtype=float32)}\n",
      "{'Training Loss': array(0.05846026, dtype=float32)}\n",
      "{'Training Loss': array(0.05223525, dtype=float32)}\n",
      "{'Training Loss': array(0.04153753, dtype=float32)}\n",
      "{'Training Loss': array(0.04508903, dtype=float32)}\n",
      "{'Training Loss': array(0.04084521, dtype=float32)}\n",
      "{'Training Loss': array(0.03524305, dtype=float32)}\n",
      "{'Training Loss': array(0.03669402, dtype=float32)}\n",
      "{'Training Loss': array(0.03293456, dtype=float32)}\n",
      "{'Training Loss': array(0.02970051, dtype=float32)}\n",
      "{'Training Loss': array(0.03020112, dtype=float32)}\n",
      "{'Training Loss': array(0.02451224, dtype=float32)}\n",
      "{'Training Loss': array(0.02653441, dtype=float32)}\n",
      "{'Training Loss': array(0.02438584, dtype=float32)}\n",
      "{'Training Loss': array(0.02289865, dtype=float32)}\n",
      "{'Training Loss': array(0.02056148, dtype=float32)}\n",
      "{'Training Loss': array(0.02226644, dtype=float32)}\n",
      "{'Training Loss': array(0.02052518, dtype=float32)}\n",
      "{'Training Loss': array(0.02110981, dtype=float32)}\n",
      "{'Training Loss': array(0.01861139, dtype=float32)}\n",
      "{'Training Loss': array(0.02016343, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 416.85894775390625\n",
      "Eval_StdReturn : 94.71495819091797\n",
      "Eval_MaxReturn : 646.9876708984375\n",
      "Eval_MinReturn : 308.50433349609375\n",
      "Eval_AverageEpLen : 70.73333333333333\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 75.9061028957367\n",
      "Training Loss : 0.01922936737537384\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_08-11-2020_00-50-16\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.1107526, dtype=float32)}\n",
      "{'Training Loss': array(0.08891451, dtype=float32)}\n",
      "{'Training Loss': array(0.09406006, dtype=float32)}\n",
      "{'Training Loss': array(0.08971193, dtype=float32)}\n",
      "{'Training Loss': array(0.09019943, dtype=float32)}\n",
      "{'Training Loss': array(0.09901816, dtype=float32)}\n",
      "{'Training Loss': array(0.07857466, dtype=float32)}\n",
      "{'Training Loss': array(0.09250061, dtype=float32)}\n",
      "{'Training Loss': array(0.09175465, dtype=float32)}\n",
      "{'Training Loss': array(0.0851729, dtype=float32)}\n",
      "{'Training Loss': array(0.08770096, dtype=float32)}\n",
      "{'Training Loss': array(0.0882723, dtype=float32)}\n",
      "{'Training Loss': array(0.07632992, dtype=float32)}\n",
      "{'Training Loss': array(0.08670624, dtype=float32)}\n",
      "{'Training Loss': array(0.08786112, dtype=float32)}\n",
      "{'Training Loss': array(0.08261158, dtype=float32)}\n",
      "{'Training Loss': array(0.07446103, dtype=float32)}\n",
      "{'Training Loss': array(0.08463014, dtype=float32)}\n",
      "{'Training Loss': array(0.07191277, dtype=float32)}\n",
      "{'Training Loss': array(0.08693556, dtype=float32)}\n",
      "{'Training Loss': array(0.07952631, dtype=float32)}\n",
      "{'Training Loss': array(0.08212422, dtype=float32)}\n",
      "{'Training Loss': array(0.08466227, dtype=float32)}\n",
      "{'Training Loss': array(0.07939883, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 317.087646484375\n",
      "Eval_StdReturn : 40.026885986328125\n",
      "Eval_MaxReturn : 524.452392578125\n",
      "Eval_MinReturn : 230.69886779785156\n",
      "Eval_AverageEpLen : 59.785714285714285\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 86.16292667388916\n",
      "Training Loss : 0.08106904476881027\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.005 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_08-11-2020_00-51-45\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.07113983, dtype=float32)}\n",
      "{'Training Loss': array(0.04950904, dtype=float32)}\n",
      "{'Training Loss': array(0.04164561, dtype=float32)}\n",
      "{'Training Loss': array(0.03635455, dtype=float32)}\n",
      "{'Training Loss': array(0.03789717, dtype=float32)}\n",
      "{'Training Loss': array(0.0347064, dtype=float32)}\n",
      "{'Training Loss': array(0.0272093, dtype=float32)}\n",
      "{'Training Loss': array(0.03219686, dtype=float32)}\n",
      "{'Training Loss': array(0.03455574, dtype=float32)}\n",
      "{'Training Loss': array(0.02644935, dtype=float32)}\n",
      "{'Training Loss': array(0.02512901, dtype=float32)}\n",
      "{'Training Loss': array(0.02707524, dtype=float32)}\n",
      "{'Training Loss': array(0.02522172, dtype=float32)}\n",
      "{'Training Loss': array(0.02669057, dtype=float32)}\n",
      "{'Training Loss': array(0.02285906, dtype=float32)}\n",
      "{'Training Loss': array(0.02786255, dtype=float32)}\n",
      "{'Training Loss': array(0.02008486, dtype=float32)}\n",
      "{'Training Loss': array(0.02295023, dtype=float32)}\n",
      "{'Training Loss': array(0.02216887, dtype=float32)}\n",
      "{'Training Loss': array(0.02139848, dtype=float32)}\n",
      "{'Training Loss': array(0.02252214, dtype=float32)}\n",
      "{'Training Loss': array(0.02223778, dtype=float32)}\n",
      "{'Training Loss': array(0.02272357, dtype=float32)}\n",
      "{'Training Loss': array(0.02105291, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 315.3011779785156\n",
      "Eval_StdReturn : 49.69074249267578\n",
      "Eval_MaxReturn : 537.5669555664062\n",
      "Eval_MinReturn : 227.01422119140625\n",
      "Eval_AverageEpLen : 56.764044943820224\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 84.64678025245667\n",
      "Training Loss : 0.021087080240249634\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.0005 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q1_bc_human_Humanoid-v2_08-11-2020_00-35-20\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=376, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=17, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(1.3372201, dtype=float32)}\n",
      "{'Training Loss': array(0.24904662, dtype=float32)}\n",
      "{'Training Loss': array(0.22162661, dtype=float32)}\n",
      "{'Training Loss': array(0.21992211, dtype=float32)}\n",
      "{'Training Loss': array(0.24818039, dtype=float32)}\n",
      "{'Training Loss': array(0.22765276, dtype=float32)}\n",
      "{'Training Loss': array(0.2300015, dtype=float32)}\n",
      "{'Training Loss': array(0.2626008, dtype=float32)}\n",
      "{'Training Loss': array(0.22305498, dtype=float32)}\n",
      "{'Training Loss': array(0.26978627, dtype=float32)}\n",
      "{'Training Loss': array(0.2462289, dtype=float32)}\n",
      "{'Training Loss': array(0.24823655, dtype=float32)}\n",
      "{'Training Loss': array(0.23498721, dtype=float32)}\n",
      "{'Training Loss': array(0.25015107, dtype=float32)}\n",
      "{'Training Loss': array(0.24518159, dtype=float32)}\n",
      "{'Training Loss': array(0.22599655, dtype=float32)}\n",
      "{'Training Loss': array(0.23273359, dtype=float32)}\n",
      "{'Training Loss': array(0.25799528, dtype=float32)}\n",
      "{'Training Loss': array(0.24067475, dtype=float32)}\n",
      "{'Training Loss': array(0.22726744, dtype=float32)}\n",
      "{'Training Loss': array(0.2569524, dtype=float32)}\n",
      "{'Training Loss': array(0.22794665, dtype=float32)}\n",
      "{'Training Loss': array(0.23011672, dtype=float32)}\n",
      "{'Training Loss': array(0.21383858, dtype=float32)}\n",
      "{'Training Loss': array(0.22624643, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 298.5030517578125\n",
      "Eval_StdReturn : 63.1663818359375\n",
      "Eval_MaxReturn : 576.6903076171875\n",
      "Eval_MinReturn : 174.58328247070312\n",
      "Eval_AverageEpLen : 57.31460674157304\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 81.26894021034241\n",
      "Training Loss : 0.2504897713661194\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Humanoid.pkl --env_name Humanoid-v2 --exp_name bc_human --n_iter 1 --expert_data cs285\\expert_data\\expert_data_Humanoid-v2.pkl --video_log_freq -1 --num_agent_train_steps_per_iter 50000 -lr 0.05 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "########################\n",
      "logging outputs to  C:\\Users\\vince\\homework_fall2020\\hw1\\cs285\\scripts\\../../data\\q2_dagger_ant_Ant-v2_08-11-2020_01-05-24\n",
      "########################\n",
      "Using GPU id 0\n",
      "Sequential(\n",
      "  (linear1): Linear(in_features=111, out_features=64, bias=True)\n",
      "  (activation1): Tanh()\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (activation2): Tanh()\n",
      "  (linear3): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (activation3): Identity()\n",
      ")\n",
      "Loading expert policy from... cs285\\policies\\experts\\Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.10836899, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4767.76318359375\n",
      "Eval_StdReturn : 87.17462158203125\n",
      "Eval_MaxReturn : 4916.05419921875\n",
      "Eval_MinReturn : 4650.20263671875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4713.6533203125\n",
      "Train_StdReturn : 12.196533203125\n",
      "Train_MaxReturn : 4725.849609375\n",
      "Train_MinReturn : 4701.45654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 5.1221678256988525\n",
      "Training Loss : 0.001377078820951283\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00154833, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4751.83447265625\n",
      "Eval_StdReturn : 66.36245727539062\n",
      "Eval_MaxReturn : 4814.064453125\n",
      "Eval_MinReturn : 4638.98095703125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4691.3291015625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4691.3291015625\n",
      "Train_MinReturn : 4691.3291015625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 10.728105545043945\n",
      "Training Loss : 0.0007450517732650042\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00093586, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4671.71142578125\n",
      "Eval_StdReturn : 105.21333312988281\n",
      "Eval_MaxReturn : 4796.98681640625\n",
      "Eval_MinReturn : 4516.791015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4699.03076171875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4699.03076171875\n",
      "Train_MinReturn : 4699.03076171875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 16.498465061187744\n",
      "Training Loss : 0.0006044440087862313\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00084143, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4667.7744140625\n",
      "Eval_StdReturn : 57.8217887878418\n",
      "Eval_MaxReturn : 4735.357421875\n",
      "Eval_MinReturn : 4572.3427734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4711.02294921875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4711.02294921875\n",
      "Train_MinReturn : 4711.02294921875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 22.292245864868164\n",
      "Training Loss : 0.00107271084561944\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00080925, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4786.41015625\n",
      "Eval_StdReturn : 77.4076919555664\n",
      "Eval_MaxReturn : 4898.09375\n",
      "Eval_MinReturn : 4660.357421875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 2969.23779296875\n",
      "Train_StdReturn : 1911.21728515625\n",
      "Train_MaxReturn : 4880.455078125\n",
      "Train_MinReturn : 1058.0205078125\n",
      "Train_AverageEpLen : 619.0\n",
      "Train_EnvstepsSoFar : 4238\n",
      "TimeSinceStart : 28.04873251914978\n",
      "Training Loss : 0.0006393500370904803\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00055152, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4704.212890625\n",
      "Eval_StdReturn : 90.13605499267578\n",
      "Eval_MaxReturn : 4843.7919921875\n",
      "Eval_MinReturn : 4594.54052734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4717.1728515625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4717.1728515625\n",
      "Train_MinReturn : 4717.1728515625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 5238\n",
      "TimeSinceStart : 33.43414330482483\n",
      "Training Loss : 0.00046161405043676496\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00047606, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4746.7001953125\n",
      "Eval_StdReturn : 61.3619499206543\n",
      "Eval_MaxReturn : 4825.7294921875\n",
      "Eval_MinReturn : 4647.5146484375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3577.84130859375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3577.84130859375\n",
      "Train_MinReturn : 3577.84130859375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 6238\n",
      "TimeSinceStart : 39.09164214134216\n",
      "Training Loss : 0.00039150979137048125\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00061533, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4742.9814453125\n",
      "Eval_StdReturn : 81.60968017578125\n",
      "Eval_MaxReturn : 4832.1748046875\n",
      "Eval_MinReturn : 4612.060546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4731.60400390625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4731.60400390625\n",
      "Train_MinReturn : 4731.60400390625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 7238\n",
      "TimeSinceStart : 44.8076434135437\n",
      "Training Loss : 0.0004221780982334167\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00038534, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4807.5703125\n",
      "Eval_StdReturn : 93.08290100097656\n",
      "Eval_MaxReturn : 4906.3662109375\n",
      "Eval_MinReturn : 4674.1904296875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4730.84765625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4730.84765625\n",
      "Train_MinReturn : 4730.84765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 8238\n",
      "TimeSinceStart : 51.030959129333496\n",
      "Training Loss : 0.0004125149571336806\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "{'Training Loss': array(0.00044527, dtype=float32)}\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4742.88134765625\n",
      "Eval_StdReturn : 92.75971221923828\n",
      "Eval_MaxReturn : 4820.10498046875\n",
      "Eval_MinReturn : 4573.06005859375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4714.55126953125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4714.55126953125\n",
      "Train_MinReturn : 4714.55126953125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 9238\n",
      "TimeSinceStart : 57.02106499671936\n",
      "Training Loss : 0.00038901882362551987\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cs285\\scripts\\run_hw1.py --expert_policy_file cs285\\policies\\experts\\Ant.pkl --env_name Ant-v2 --exp_name dagger_ant --n_iter 10 --do_dagger --expert_data cs285\\expert_data\\expert_data_Ant-v2.pkl --video_log_freq -1 --ep_len 1000 --eval_batch_size 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
